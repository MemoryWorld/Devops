# 免费算力+一键远程调用！魔搭MCP市场，开启AI服务新纪元

```
MCP（Model Context Protocol）让LLM轻松获取多渠道信息，全面释放强大潜能。
```

## 当前LLM的局限

现在的大语言模型（LLM）已经展现出强大的能力，许多人的日常生活和工作都已无法离开它们。从撰写文案到代码生成，从语言翻译到复杂问题的解答，LLM正在各个领域发挥着重要作用。

然而，这些模型也存在明显的局限性。首先，大多数LLM的知识库是基于训练时的数据构建的，无法频繁更新。这意味着它们对最新的信息、事件或技术进展可能无能为力。其次，通过一般的网络搜索方式，用户通常很难精准获取所需的信息，尤其是当问题涉及到多渠道或复杂上下文时。这种信息获取的局限性，限制了LLM在某些场景下的实用性。

## MCP也没有那么容易使用

虽然MCP可以非常灵活地根据用户需要，为大模型提供各式各样的信息，从而有效解决上述的弊端，但其使用上仍然存在一定的门槛。

目前，绝大多数MCP服务都需要在本地架设。这意味着用户需要熟悉其本地运行环境的搭建过程。这对许多一般用户来说依然是一个挑战。这种技术门槛在一定程度上限制了MCP的普及和广泛应用。

## 魔搭，让这一切变得更加简单

2025年4月15日，魔搭（ModelScope）平台推出了[MCP广场](https://modelscope.cn/mcp)，成为中文开源社区中最大的MCP服务集市。该广场目前已上线近1500种MCP服务器，涵盖搜索、地图、文件系统、开发者工具等热门领域。魔搭平台提供简单易用的工具和调试环境MCP实验场，开发者可在1分钟内搭建所需的MCP服务。这些服务支持云上托管或本地部署，均可开放集成至第三方平台，显著降低了开发者使用MCP服务的门槛，推动了AI应用的创新和落地。

简单来说，用户可以直接在市场上选择自己喜欢的MCP服务，然后一键获取调用他们的SSE URL，就可以在自己支持MCP的客户端中使用这些服务了。

## 一个例子

小编这里展示了一个魔搭MCP广场的使用案例，让您能对这一简单的流程有一个直观的认知。

假如，我希望让LLM能够实时查询当前Arxiv上的文章来帮助我了解某一个方向的发展现状，我可以在时长上选择`arxiv-mcp-server`:

![alt text](image-3.png)

接下来，我只需要跟着魔搭的引导，在云端一键启用该服务，就能获取专用的调用URL：

![alt text](image-4.png)

有了这个链接，我就可以在支持MCP的客户端中直接加入并调用该服务端了。假如我希望了解当前大语言模型领域的发展：

![alt text](image.png)

可以看到，MCP服务端已被成功调用。不一会儿，我想要的报告就被写好了：

![alt text](image-1.png)

## 小结

当前，魔搭的MCP广场已经上线了超过160个可在线一键部署和调用的MCP服务端。随着MCP的发展和普及，MCP的应用场景将会更加广泛。无论是科研、教育、商业还是日常生活，MCP都能为用户提供更加便捷和高效的服务。未来，随着更多开发者的加入和技术的不断进步，MCP有望成为AI服务领域的核心技术之一，为人类社会带来更多的创新和便利。